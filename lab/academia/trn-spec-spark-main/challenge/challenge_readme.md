### Data Engineer with Spark 3.0 [Challenge]

#### Instructions:

First of all, congratulations in finish the databricks course, was really good have you with us, but now is time to for you to measure your knowledge learned in our training, so lets do this!
Challenge will measure your knowledge and understanding of Big Data, how properly use spark in data pipelines and you will be free to do as you like, the only wrong answer is not do it, so be keep with us in this!

##### Notes:
* Explore a real use-case and apply your knowledge in coding.
* Best practices of data lake is mandatory.
* Delta architecture and data lakehouse are mandatory but not for all notebooks and application, only when fits right.
* Deliver a data pipeline with spark in one week, 7 days counting from next Tuesday after end of the course.
* First 2 to finish and deliver operacional and functional pipeline will receive a reward from One Way Solution Team.
* The code and notebooks must be deliver to One Way Solution Team, we will reply to all in 1 to 2 weeks after receive, so 1 shot only okay
* Code and Notebooks will have to be well documented, you can create readme.md with documentation if you like or put inside the code, be clear in comments but don't write a book, make it simple as possible.
* We will receive the codes after 7 days but this one will not receive a gift, but will be analyze and if needed will be point what can be fix, the idea of the challenge is exercise what he learn in the training so don't feel pressure and work in your pace.

##### Deliverables:
Create a end to end pipeline with spark using the knowledge you acquire:
* 1 notebook with batching processing
* 1 notebook with streaming processing
* 1 spark application development locally
* the business rules must be apply inside of each notebook and spark application

All data information are fictional as the use cases and business rules, idea is to create a test environment to apply the knowledge similar to real life.

#### Important notes:

You can choose one or more business segments to apply in your pipeline can be:
* one business rule for all notebooks and application
* different business rules for each notebook and application
* all data is in https://github.com/luanmorenomaciel/de-apache-spark/tree/main/files/landing-zone 


Important that you choose after you access the data and see which you feel more comfortable and excited to build the pipelines.

#### Measure skills:

Coding, functionality & Documentation [simpler the better] - 70%
New insights for each pipeline [give us something we don't see] - 30%

#### Deadline

* 7 Days [reward]
* 90 days - review and considerations [1 to 2 weeks to reply]
* After 90 days, we will need one month to reply


#### Reward

This is the most expected session of the document, 2 first students to deliver the challenge in 7 days with 90% ~ to 100% in measure skills will receive

* 50% discount in 1 One Way Solution Training of your choosing [expire in 1 year from now, end of may 2022] (trainings with discount will not be elegible for this reward)
